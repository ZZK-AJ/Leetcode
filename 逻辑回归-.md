1、逻辑回归与线性回归的联系与区别
逻辑回归算法实际上是一种分类算法，它适用于标签y取值离散的情况，该算法的输出值永远在0到1之间。

线性回归算法假设特征和结果满足线性关系，适用于预测连续输出值的情况。

2、逻辑回归
逻辑回归模型的假设是：



其中，X代表特征向量，g代表逻辑函数是一个常用的逻辑函数为S形函数，公式为：



该函数图像为：



ℎ𝜃(𝑥)的作用是，对于给定的输入变量，根据选择的参数计算输出变量=1 的可能性（estimated probablity）即

                                                                                     ℎ𝜃(𝑥) = 𝑃(𝑦 = 1|𝑥; 𝜃)

在逻辑回归中，我们预测：
当ℎ𝜃(𝑥) >= 0.5时，预测 𝑦 = 1。
当ℎ𝜃(𝑥) < 0.5时，预测 𝑦 = 0 。

根据上面绘制出的 S 形函数图像，我们知道当
𝑧 >= 0 时 𝑔(𝑧)> = 0.5
𝑧 < 0 时 𝑔(𝑧) < 0.5
又 𝑧 = 𝑥 ，即：
𝑥 >= 0 时，预测 𝑦 = 1
𝑥 < 0 时，预测 𝑦 = 0

3、逻辑回归损失函数推导及优化
我要定义用来拟合参数的优化目标或者叫代价函数，



对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）。



 

这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。

线性回归的代价函数为：



我们重新定义逻辑回归的代价函数为：



其中：



推导过程如下：



梯度下降并不是我们可以使用的唯一算法，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数𝐽(𝜃)和偏导数项两个项的话，那么这些算法就是为我们优化代价函数的不同方法，共轭梯度法 BFGS (变尺度法) 和L-BFGS (限制变尺度法) 就是其中一些更高级的优化算法，它们需要有一种方法来计算 𝐽(𝜃)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。

4、正则化的逻辑回归模型


自己计算导数同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：



要最小化该代价函数，通过求导，得出梯度下降算法为：



注意：
1. 虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样，但由于两者的ℎ𝜃(𝑥)不同所以还是有很大差别。
2. 不参与其中的任何一个正则化。

5、模型评估指标







ROC曲线，横坐标为false positive rate(FPR)，纵坐标为true positive rate（TPR），由混淆矩阵可得到横纵轴的计算公式：



代表分类器预测的正类中实际正实例占所有正实例的比例。直观上代表能将正例分对的概率。



代表分类器预测的正类中实际负实例占所有负实例的比例。直观上代表将负类错分为正例的概率。



6、逻辑回归优缺点
优点：
逻辑回归是一种被人们广泛使用的算法，因为它非常高效，不需要太大的计算量，又通俗易懂，不需要缩放输入特征，且很容易调整，并且输出校准好的预测概率。与线性回归一样，当你去掉与输出变量无关的属性以及相似度高的属性时，逻辑回归效果确实会更好。因此特征处理在 逻辑回归和线性回归的性能方面起着重要的作用。

逻辑回归的另一个优点是它非常容易实现，且训练起来很高效。

缺点：
我们不能用逻辑回归来解决非线性问题，因为它的决策边界是线性的，且高度依赖正确的数据表示。


7、样本不均衡问题解决办法
几个可能的解决方法

扩大数据集
尝试其它评价指标
对数据集进行重采样
尝试产生人工数据样本
尝试不同的分类算法
尝试对模型进行惩罚
尝试一个新的角度理解问题
尝试创新
参考 https://blog.csdn.net/heyongluoyao8/article/details/49408131

8、sklearn参数
参考 https://blog.csdn.net/fuqiuai/article/details/79495865
--------------------- 
版权声明：本文为CSDN博主「Karin Su」的原创文章，遵循CC 4.0 by-sa版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Karinsu/article/details/98979816